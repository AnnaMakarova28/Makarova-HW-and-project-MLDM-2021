{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLDM Project Learning to learn",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt4ll6iRmxf3"
      },
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import functools\n",
        "import operator\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torch.optim as optim\n",
        "\n",
        "import itertools\n",
        "from itertools import islice\n",
        "from collections import OrderedDict\n",
        "\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IV5BHegV8Wl"
      },
      "source": [
        "Two-layer LSTMs with 20 hidden units in each layer.\n",
        "\n",
        "Each optimizer is trained using truncated back propagation through time\n",
        "The minimization is performed using ADAM with a learning rate chosen by random search.\n",
        "\n",
        "Early stopping when training the optimizer in order to avoid overfitting the optimizer.\n",
        "\n",
        "After each epoch (some fixed number of learning steps) freeze the optimizer parameters and evaluate its performance.\n",
        "\n",
        "\n",
        "Trained optimizers are compared with standard optimizers used in Deep Learning: SGD, RMSprop, ADAM, and Nesterovâ€™s accelerated gradient (NAG)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e-SEl6xQISJ"
      },
      "source": [
        "This paper describes a method of NN work optimization with LSTM (long short-term memory).\n",
        "\n",
        "Optimizer has been applied differently in 4 variants:\n",
        "\n",
        "1. To optimize random quadratic functions to the other functions from the same distribution.\n",
        "\n",
        "2. To optimize classic MLP (with different architectures) on MNIST\n",
        "\n",
        "3. To optimize claasification on CIFAR\n",
        "\n",
        "4. To implement the optimizer in Neural Art to mix images style\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp23Ajsn8NVX"
      },
      "source": [
        "# helpers\n",
        "\n",
        "def w(v):\n",
        "    if torch.cuda.is_available():\n",
        "        return v.cuda()\n",
        "    return v\n",
        "\n",
        "def detach_var(v):\n",
        "    var = w(Variable(v.data, requires_grad=True))\n",
        "    var.retain_grad()\n",
        "    return var\n",
        "\n",
        "def rsetattr(obj, attr, val):\n",
        "    pre, _, post = attr.rpartition('.')\n",
        "    return setattr(rgetattr(obj, pre) if pre else obj, post, val)\n",
        "\n",
        "def rgetattr(obj, attr, *args):\n",
        "    def _getattr(obj, attr):\n",
        "        return getattr(obj, attr, *args)\n",
        "    return functools.reduce(_getattr, [obj] + attr.split('.'))\n",
        "\n",
        "def to_var(x, requires_grad=True):\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "    return Variable(x, requires_grad=requires_grad)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ8eYJJ28Rbf"
      },
      "source": [
        "# loss\n",
        "\n",
        "# here, we define the target tasks for optimizees\n",
        "\n",
        "# we build an optimizer which finds vector theta (10-element) \n",
        "# theta * W (10x10 matrix) supposed to be as close as possible to y (another 10-elem vector\n",
        "# from our distribution)\n",
        "\n",
        "# compute error as Mean Squarred Error\n",
        "\n",
        "class QuadraticLoss:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.W = w(Variable(torch.randn(10, 10)))\n",
        "        self.y = w(Variable(torch.randn(10)))\n",
        "        \n",
        "    def get_loss(self, theta):\n",
        "        return torch.sum((self.W.matmul(theta) - self.y)**2)\n",
        "\n",
        "class MNISTLoss: \n",
        "    def __init__(self, training=True):\n",
        "        try:\n",
        "            os.mkdir('data')\n",
        "        except:\n",
        "            pass\n",
        "        dataset = datasets.MNIST(\n",
        "            'data', train=True, download=True,\n",
        "            transform=torchvision.transforms.ToTensor()\n",
        "        )\n",
        "        indices = list(range(len(dataset)))\n",
        "        np.random.RandomState(10).shuffle(indices)\n",
        "        if training:\n",
        "            indices = indices[:len(indices) // 2]\n",
        "        else:\n",
        "            indices = indices[len(indices) // 2:]\n",
        "\n",
        "        # sample MNIST dataset to batches with 128 size    \n",
        "        self.loader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_size=128,\n",
        "            sampler=torch.utils.data.sampler.SubsetRandomSampler(indices))\n",
        "\n",
        "        self.batches = []\n",
        "        self.cur_batch = 0\n",
        "        \n",
        "    def sample(self):\n",
        "        if self.cur_batch >= len(self.batches):\n",
        "            self.batches = []\n",
        "            self.cur_batch = 0\n",
        "            for b in self.loader:\n",
        "                self.batches.append(b)\n",
        "        batch = self.batches[self.cur_batch]\n",
        "        self.cur_batch += 1\n",
        "        return batch"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Hj2qMk8RYO"
      },
      "source": [
        "# meta module\n",
        "class MetaModule(nn.Module):\n",
        "    def parameters(self):\n",
        "       for name, param in self.named_params(self):\n",
        "            yield param\n",
        "\n",
        "    def named_parameters(self):\n",
        "       for name, param in self.named_params(self):\n",
        "            yield name, param\n",
        "    \n",
        "    def named_leaves(self):\n",
        "        return []\n",
        "    \n",
        "    def named_submodules(self):\n",
        "        return []\n",
        "    \n",
        "    def named_params(self, curr_module=None, memo=None, prefix=''):       \n",
        "        if memo is None:\n",
        "            memo = set()\n",
        "\n",
        "        if hasattr(curr_module, 'named_leaves'):\n",
        "            for name, p in curr_module.named_leaves():\n",
        "                if p is not None and p not in memo:\n",
        "                    memo.add(p)\n",
        "                    yield prefix + ('.' if prefix else '') + name, p\n",
        "        else:\n",
        "            for name, p in curr_module._parameters.items():\n",
        "                if p is not None and p not in memo:\n",
        "                    memo.add(p)\n",
        "                    yield prefix + ('.' if prefix else '') + name, p\n",
        "                    \n",
        "        for mname, module in curr_module.named_children():\n",
        "            submodule_prefix = prefix + ('.' if prefix else '') + mname\n",
        "            for name, p in self.named_params(module, memo, submodule_prefix):\n",
        "                yield name, p\n",
        "    \n",
        "    def update_params(self, lr_inner, first_order=False, source_params=None, detach=False):\n",
        "        if source_params is not None:\n",
        "            for tgt, src in zip(self.named_params(self), source_params):\n",
        "                name_t, param_t = tgt\n",
        "                grad = src\n",
        "                if first_order:\n",
        "                    grad = to_var(grad.detach().data)\n",
        "                tmp = param_t - lr_inner * grad\n",
        "                self.set_param(self, name_t, tmp)\n",
        "        else:\n",
        "            for name, param in self.named_params(self):\n",
        "                if not detach:\n",
        "                    grad = param.grad\n",
        "                    if first_order:\n",
        "                        grad = to_var(grad.detach().data)\n",
        "                    tmp = param - lr_inner * grad\n",
        "                    self.set_param(self, name, tmp)\n",
        "                else:\n",
        "                    param = param.detach_()\n",
        "                    self.set_param(self, name, param)\n",
        "\n",
        "    def set_param(self,curr_mod, name, param):\n",
        "        if '.' in name:\n",
        "            n = name.split('.')\n",
        "            module_name = n[0]\n",
        "            rest = '.'.join(n[1:])\n",
        "            for name, mod in curr_mod.named_children():\n",
        "                if module_name == name:\n",
        "                    self.set_param(mod, rest, param)\n",
        "                    break\n",
        "        else:\n",
        "            setattr(curr_mod, name, param)\n",
        "            \n",
        "    def detach_params(self):\n",
        "        for name, param in self.named_params(self):\n",
        "            self.set_param(self, name, param.detach())   \n",
        "                \n",
        "    def copy(self, other, same_var=False):\n",
        "        for name, param in other.named_params():\n",
        "            if not same_var:\n",
        "                param = to_var(param.data.clone(), requires_grad=True)\n",
        "            self.set_param(name, param)\n",
        "\n",
        "\n",
        "class MetaLinear(MetaModule):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        ignore = nn.Linear(*args, **kwargs)\n",
        "       \n",
        "        self.register_buffer('weight', to_var(ignore.weight.data, requires_grad=True))\n",
        "        self.register_buffer('bias', to_var(ignore.bias.data, requires_grad=True))\n",
        "        self.in_features = ignore.weight.size(1)\n",
        "        self.out_features = ignore.weight.size(0)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return F.linear(x, self.weight, self.bias)\n",
        "    \n",
        "    def named_leaves(self):\n",
        "        return [('weight', self.weight), ('bias', self.bias)]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXYBxBuI8RVq"
      },
      "source": [
        "# meta optimizer\n",
        "# here the optimizer of the network is built\n",
        "\n",
        "class Optimizer_LSTM(nn.Module):\n",
        "    def __init__(self, preproc=False, hidden_sz=20, preproc_factor=10.0):\n",
        "        super().__init__()\n",
        "        self.hidden_sz = hidden_sz\n",
        "        if preproc:\n",
        "            self.recurs = nn.LSTMCell(2, hidden_sz)\n",
        "        else:\n",
        "            self.recurs = nn.LSTMCell(1, hidden_sz)\n",
        "        self.recurs2 = nn.LSTMCell(hidden_sz, hidden_sz)\n",
        "        self.output = nn.Linear(hidden_sz, 1)\n",
        "        self.preproc = preproc\n",
        "        self.preproc_factor = preproc_factor\n",
        "        self.preproc_threshold = np.exp(-preproc_factor)\n",
        "        \n",
        "    def forward(self, inp, hidden, cell):\n",
        "        if self.preproc:\n",
        "            inp = inp.data\n",
        "            inp2 = w(torch.zeros(inp.size()[0], 2))\n",
        "            keep_grads = (torch.abs(inp) >= self.preproc_threshold).squeeze()\n",
        "            inp2[:, 0][keep_grads] = (torch.log(torch.abs(inp[keep_grads]) + 1e-8) / self.preproc_factor).squeeze()\n",
        "            inp2[:, 1][keep_grads] = torch.sign(inp[keep_grads]).squeeze()\n",
        "            \n",
        "            inp2[:, 0][~keep_grads] = -1\n",
        "            inp2[:, 1][~keep_grads] = (float(np.exp(self.preproc_factor)) * inp[~keep_grads]).squeeze()\n",
        "            inp = w(Variable(inp2))\n",
        "        hidden0, cell0 = self.recurs(inp, (hidden[0], cell[0]))\n",
        "        hidden1, cell1 = self.recurs2(hidden0, (hidden[1], cell[1]))\n",
        "        return self.output(hidden1), (hidden0, hidden1), (cell0, cell1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpWWnz048RTL"
      },
      "source": [
        "# optimizee\n",
        "# here the optimizers for the tasks (for the experiments with quadratic\n",
        "# functions and MNIST dataset) are built\n",
        "\n",
        "class QuadOptimizee(MetaModule):\n",
        "    def __init__(self, theta=None):\n",
        "        super().__init__()\n",
        "        self.register_buffer('theta', to_var(torch.zeros(10).cuda(), requires_grad=True))\n",
        "        \n",
        "    def forward(self, target):\n",
        "        return target.get_loss(self.theta)\n",
        "    \n",
        "    def all_named_parameters(self):\n",
        "        return [('theta', self.theta)]\n",
        "\n",
        "class QuadOptimizeeNormal(nn.Module):\n",
        "    def __init__(self, theta=None):\n",
        "        super().__init__()\n",
        "        if theta is None:\n",
        "            self.theta = nn.Parameter(torch.zeros(10))\n",
        "        else:\n",
        "            self.theta = theta\n",
        "        \n",
        "    def forward(self, target):\n",
        "        return target.get_loss(self.theta)\n",
        "    \n",
        "    def all_named_parameters(self):\n",
        "        return [('theta', self.theta)]\n",
        "\n",
        "# optimizee for MNIST is a neural network with 1 layer and 20 hidden units\n",
        "# parameter of this function is activation which is used to change\n",
        "# the activation function as it has been done in the paper\n",
        "\n",
        "def create_MNISTNet(activation):\n",
        "    class MNISTNet(MetaModule):\n",
        "        def __init__(self, layer_size=20, n_layers=1, **kwargs):\n",
        "            super().__init__()\n",
        "\n",
        "            inp_size = 28*28\n",
        "            self.layers = {}\n",
        "            for i in range(n_layers):\n",
        "                self.layers[f'mat_{i}'] = MetaLinear(inp_size, layer_size)\n",
        "                inp_size = layer_size\n",
        "\n",
        "            self.layers['final_mat'] = MetaLinear(inp_size, 10)\n",
        "            self.layers = nn.ModuleDict(self.layers)\n",
        "\n",
        "            self.activation = activation\n",
        "            self.loss = nn.NLLLoss()\n",
        "\n",
        "        def all_named_parameters(self):\n",
        "            return [(k, v) for k, v in self.named_parameters()]\n",
        "\n",
        "        def forward(self, loss):\n",
        "            inp, out = loss.sample()\n",
        "            inp = w(Variable(inp.view(inp.size()[0], 28*28)))\n",
        "            out = w(Variable(out))\n",
        "\n",
        "            cur_layer = 0\n",
        "            while f'mat_{cur_layer}' in self.layers:\n",
        "                inp = self.activation(self.layers[f'mat_{cur_layer}'](inp))\n",
        "                cur_layer += 1\n",
        "\n",
        "            inp = F.log_softmax(self.layers['final_mat'](inp), dim=1)\n",
        "            l = self.loss(inp, out)\n",
        "            return l\n",
        "    return MNISTNet"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxeJYCOJ8RRE"
      },
      "source": [
        "# training\n",
        "\n",
        "# here the one epoch training of the optimizer is built\n",
        "\n",
        "def one_step_fit_LSTM(opt_net, meta_opt, target_cls, target_to_opt, \\\n",
        "                      unroll, optim_it, out_mul, should_train=True):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        - opt_net: \n",
        "            an instance of Optimizer_LSTM class\n",
        "        - meta_opt:\n",
        "            a normal optimizer, e.g. Adam\n",
        "        - target_cls: \n",
        "            loss function, an instance of QuadraticLoss, or MNISTLoss, or other similar classes.\n",
        "        - target_to_opt:\n",
        "            an optimizee, an instance of QuadOptimizee, or MNISTNet, or other similar classes.\n",
        "        - unroll:\n",
        "            int, how often make a gradient step for LSTM-based optimizer (measuring in training steps of optimizee)\n",
        "        - optim_it:\n",
        "            int, how many steps train an optimizee\n",
        "        - out_mul:\n",
        "            float, a multiplier of the LSTM-based optimizer outputs (usual learning rate in some sense)\n",
        "        - should_train: \n",
        "            bool, whether the LSTM-based optimizer is trained or not.\n",
        "    Returns:\n",
        "        - all_losses_ever: \n",
        "            list, all losses of the optimizee for optim_it steps.\n",
        "    \"\"\"\n",
        "    if should_train:\n",
        "        opt_net.train()\n",
        "    else:\n",
        "        opt_net.eval()\n",
        "        unroll = 1\n",
        "    \n",
        "    # Initialization\n",
        "    target = target_cls(training=should_train)\n",
        "    optimizee = w(target_to_opt())\n",
        "    n_params = 0\n",
        "    for name, p in optimizee.all_named_parameters():\n",
        "        n_params += int(np.prod(p.size()))\n",
        "    # Hidden and cell states for LSTM\n",
        "    hidden_states = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
        "    cell_states = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
        "    all_losses_ever = []\n",
        "    if should_train:\n",
        "        meta_opt.zero_grad()\n",
        "    all_losses = None\n",
        "    # Usual training\n",
        "    for iteration in range(1, optim_it + 1):\n",
        "        loss = optimizee(target)\n",
        "                    \n",
        "        if all_losses is None:\n",
        "            all_losses = loss\n",
        "        else:\n",
        "            all_losses += loss\n",
        "        \n",
        "        all_losses_ever.append(loss.data.cpu().numpy())\n",
        "        loss.backward(retain_graph=should_train)\n",
        "        \n",
        "        # LSTM step\n",
        "        offset = 0\n",
        "        result_params = {}\n",
        "        hidden_states2 = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
        "        cell_states2 = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
        "        for name, p in optimizee.all_named_parameters():\n",
        "            cur_sz = int(np.prod(p.size()))\n",
        "            # We do this so the gradients are disconnected from the graph but we still get\n",
        "            # gradients from the rest\n",
        "            gradients = detach_var(p.grad.view(cur_sz, 1))\n",
        "            updates, new_hidden, new_cell = opt_net(\n",
        "                gradients,\n",
        "                [h[offset:offset+cur_sz] for h in hidden_states],\n",
        "                [c[offset:offset+cur_sz] for c in cell_states]\n",
        "            )\n",
        "            for i in range(len(new_hidden)):\n",
        "                hidden_states2[i][offset:offset+cur_sz] = new_hidden[i]\n",
        "                cell_states2[i][offset:offset+cur_sz] = new_cell[i]\n",
        "            result_params[name] = p + updates.view(*p.size()) * out_mul\n",
        "            result_params[name].retain_grad()\n",
        "            \n",
        "            offset += cur_sz\n",
        "        # Optimization of LSTM     \n",
        "        if iteration % unroll == 0:\n",
        "            if should_train:\n",
        "                meta_opt.zero_grad()\n",
        "                all_losses.backward()\n",
        "                meta_opt.step()\n",
        "                \n",
        "            all_losses = None\n",
        "\n",
        "            optimizee = w(target_to_opt())\n",
        "            optimizee.load_state_dict(result_params)\n",
        "            optimizee.zero_grad()\n",
        "            hidden_states = [detach_var(v) for v in hidden_states2]\n",
        "            cell_states = [detach_var(v) for v in cell_states2]\n",
        "            \n",
        "        else:\n",
        "            for name, p in optimizee.all_named_parameters():\n",
        "                rsetattr(optimizee, name, result_params[name])\n",
        "            assert len(list(optimizee.all_named_parameters()))\n",
        "            hidden_states = hidden_states2\n",
        "            cell_states = cell_states2\n",
        "    return all_losses_ever\n",
        "\n",
        "def fit(target_cls, target_to_opt, model_name, preproc=False, unroll=20, \\\n",
        "        optim_it=100, n_epochs=20, n_tests=100, lr=0.001, out_mul=1.0, norm=True):\n",
        "    \"\"\"\n",
        "    This function is used to train the model-based optimizers\n",
        "    Parameters:\n",
        "        - target_cls: \n",
        "            loss function, an instance of QuadraticLoss, or MNISTLoss, or other similar classes.\n",
        "        - target_to_opt:\n",
        "            an optimizee, an instance of QuadOptimizee, or MNISTNet, or other similar classes.\n",
        "        - model_name:\n",
        "            str, 'LSTM' or 'HNN' only\n",
        "        - preproc:\n",
        "            bool, whether to use preprocessing in model-based optimizers or not. \n",
        "            Usually, it is used with neural networks optimizees, details in meta_optimizer.py\n",
        "        - unroll:\n",
        "            int, how often make a gradient step for model-based optimizer (measuring in training steps of optimizee)\n",
        "        - optim_it:\n",
        "            int, how many steps train in one epoch\n",
        "        - n_epochs:\n",
        "            int, number of epochs to train optimizers\n",
        "        - n_tests:\n",
        "            int, how many test steps to do after one epoch of training\n",
        "        - lr:\n",
        "            float, learning rate of the optimizers that train model-based optimizers\n",
        "        - out_mul:\n",
        "            float, a multiplier of the model-based optimizer outputs (usual learning rate in some sense)\n",
        "    Returns:\n",
        "        - best_loss: \n",
        "            float, the best test loss\n",
        "        - best_net:\n",
        "            an instance of Optimizer_LSTM or Optimizer_HNN classes which corresponds to best_loss\n",
        "    \"\"\"\n",
        "    opt_net = w(Optimizer_LSTM(preproc=preproc))     \n",
        "    meta_opt = optim.Adam(opt_net.parameters(), lr=lr)\n",
        "    best_net = None\n",
        "    best_loss = np.inf\n",
        "    \n",
        "    for _ in range(n_epochs):\n",
        "        for _ in range(20):\n",
        "          one_step_fit_LSTM(opt_net, meta_opt, target_cls, target_to_opt, \\\n",
        "                            unroll, optim_it, out_mul, should_train=True)\n",
        "          loss = np.mean([np.mean(one_step_fit_LSTM(opt_net, meta_opt, target_cls, target_to_opt, \\\n",
        "                                                    unroll, optim_it, out_mul, should_train=False)) for _ in range(n_tests)])\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_net = copy.deepcopy(opt_net.state_dict())\n",
        "    return best_loss, best_net\n",
        "\n",
        "def fit_normal(target_cls, target_to_opt, opt_class, lr, n_tests=100, n_epochs=100, **kwargs):\n",
        "    \"\"\"\n",
        "    This function is used to train optimizees by normal optimizers\n",
        "    Parameters:\n",
        "        - target_cls: \n",
        "            loss function, an instance of QuadraticLoss, or MNISTLoss, or other similar classes.\n",
        "        - target_to_opt:\n",
        "            an optimizee, an instance of QuadOptimizee, or MNISTNet, or other similar classes.\n",
        "        - opt_class:\n",
        "            an optimizer, for example Adam or SGD (from torch.optim)\n",
        "        - n_epochs:\n",
        "            int, number of epochs to train optimizers\n",
        "        - n_tests:\n",
        "            int, how many test steps to do after one epoch of training\n",
        "    Returns:\n",
        "        - results: \n",
        "            list, a list of all loss values\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for i in range(n_tests):\n",
        "        target = target_cls(training=False)\n",
        "        optimizee = w(target_to_opt())\n",
        "        optimizer = opt_class(optimizee.parameters(), lr=lr, **kwargs)\n",
        "        total_loss = []\n",
        "        for _ in range(n_epochs):\n",
        "            loss = optimizee(target)\n",
        "            \n",
        "            total_loss.append(loss.data.cpu().numpy())\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        results.append(total_loss)\n",
        "    return results"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHEEAQhH8RO5",
        "outputId": "a5efcfb3-7438-445f-abd5-56d8c5bfa1aa"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "sns.set(color_codes=True)\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swpDimX0-vZb"
      },
      "source": [
        "# 1. Quadratic functions\n",
        "We build an optimizee which finds vector theta (10-element) theta * W (10x10 matrix) supposed to be as close as possible to y (another 10-elem vector from our distribution).\n",
        "\n",
        "Then, we apply the optimizer to this quadratic optimizee."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDU7A_xL8RLr"
      },
      "source": [
        "loss_LSTM, quad_optimizer_LSTM = fit(QuadraticLoss, QuadOptimizee, 'LSTM', unroll=20, optim_it=100, lr=0.003,\\\n",
        "                                          n_tests=50, n_epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY7yv-iu8RD-"
      },
      "source": [
        "print('Best loss of LSTM = ', loss_LSTM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSFkVBsg8Q4J"
      },
      "source": [
        "fit_data = np.zeros((100, 100, 6))\n",
        "\n",
        "opt = w(Optimizer_LSTM())\n",
        "opt.load_state_dict(quad_optimizer_LSTM)\n",
        "fit_data[:, :, 0] = np.array([one_step_fit_LSTM(opt, None, QuadraticLoss, QuadOptimizee, \\\n",
        "                                                1, 100, out_mul=1.0, should_train=False) for _ in range(100)])\n",
        "QUAD_LRS = [0.1, 0.03, 0.01, 0.01]\n",
        "NORMAL_OPTS = [(optim.Adam, {}), (optim.RMSprop, {}), (optim.SGD, {'momentum': 0.9}), (optim.SGD, {'nesterov': True, 'momentum': 0.9})]\n",
        "for i, ((opt, extra_kwargs), lr) in enumerate(zip(NORMAL_OPTS, QUAD_LRS)):\n",
        "    fit_data[:, :, 1 + i] = np.array(fit_normal(QuadraticLoss, QuadOptimizeeNormal, opt, lr=lr, **extra_kwargs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKApRJnx8QjZ"
      },
      "source": [
        "np.save('figure_one', fit_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NccKIJso9O2p"
      },
      "source": [
        "names = ['LSTM', 'ADAM', 'RMSprop', 'SGD', 'NAG']\n",
        "for i, name in enumerate(names):\n",
        "    print('{}: {:.2f}'.format(name, np.mean(fit_data, axis=0)[-1, i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9NJC6-J9PlE"
      },
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "for i, opt in enumerate(names):\n",
        "    plt.plot(np.mean(fit_data[:,:,i], axis=0), label=opt)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.yscale('log')\n",
        "plt.title('Quadratic functions')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.savefig('quad.pdf')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3c_lh5-9Pij"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaFjeJho_feW"
      },
      "source": [
        "# 2. MNIST dataset\n",
        "\n",
        "Again, we implement the optimizer to the new optimizee.\n",
        "MNIST optimizee has a single hidden layer with 20 hidden units network.\n",
        "\n",
        "Originally, the optimizer was trained with the sigmoid function and then we apply ReLU activation function to check how the optimizer works in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "polAyAVTPLik"
      },
      "source": [
        "loss_LSTM, MNIST_optimizer_LSTM = fit(MNISTLoss, create_MNISTNet(nn.Sigmoid()), 'LSTM', unroll=20, optim_it=100, lr=0.01,\\\n",
        "                                           out_mul=0.1, preproc=True, n_tests=20, n_epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heV2ETz59Pet"
      },
      "source": [
        "print('Best loss of LSTM = ', loss_LSTM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa8-PT2_AoX3"
      },
      "source": [
        "Sigmoid activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ui8XitU9Pc3"
      },
      "source": [
        "fit_data = np.zeros((100, 100, 6))\n",
        "\n",
        "opt = w(Optimizer_LSTM(preproc=True))\n",
        "\n",
        "\n",
        "opt.load_state_dict(MNIST_optimizer_LSTM)\n",
        "fit_data[:, :, 0] = np.array([one_step_fit_LSTM(opt, None, MNISTLoss, create_MNISTNet(nn.Sigmoid()), \\\n",
        "                                                1, 100, out_mul=1.0, should_train=False) for _ in range(100)])\n",
        "QUAD_LRS = [0.03, 0.01, 1.0, 1.0]\n",
        "NORMAL_OPTS = [(optim.Adam, {}), (optim.RMSprop, {}), (optim.SGD, {'momentum': 0.9}), (optim.SGD, {'nesterov': True, 'momentum': 0.9})]\n",
        "for i, ((opt, extra_kwargs), lr) in enumerate(zip(NORMAL_OPTS, QUAD_LRS)):\n",
        "    fit_data[:, :, 2 + i] = np.array(fit_normal(MNISTLoss, create_MNISTNet(nn.Sigmoid()), opt, lr=lr, **extra_kwargs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWSo8mSG9PaW"
      },
      "source": [
        "np.save('mnist_fit_data_sigmoid', fit_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7sP3aUF9PW2"
      },
      "source": [
        "names = ['LSTM', 'ADAM', 'RMSprop', 'SGD', 'NAG']\n",
        "for i, name in enumerate(names):\n",
        "    print('{}: {:.2f}'.format(name, np.mean(fit_data, axis=0)[-1, i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgir9ab7AjLO"
      },
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "for i, opt in enumerate(names):\n",
        "    plt.plot(np.mean(fit_data[:,:,i], axis=0), label=opt)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('MNIST, Sigmoid')\n",
        "plt.legend()\n",
        "plt.yscale('log')\n",
        "plt.grid()\n",
        "plt.savefig('mnist_sigmoid.pdf')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9DOKphzA6DV"
      },
      "source": [
        "ReLU activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oygSnNA0AmLi"
      },
      "source": [
        "fit_data = np.zeros((100, 100, 6))\n",
        "\n",
        "opt = w(Optimizer_LSTM(preproc=True))\n",
        "opt.load_state_dict(MNIST_optimizer_LSTM)\n",
        "fit_data[:, :, 0] = np.array([one_step_fit_LSTM(opt, None, MNISTLoss, create_MNISTNet(nn.ReLU()), \\\n",
        "\n",
        "QUAD_LRS = [0.03, 0.003, 0.3, 0.3]\n",
        "NORMAL_OPTS = [(optim.Adam, {}), (optim.RMSprop, {}), (optim.SGD, {'momentum': 0.9}), (optim.SGD, {'nesterov': True, 'momentum': 0.9})]\n",
        "for i, ((opt, extra_kwargs), lr) in enumerate(zip(NORMAL_OPTS, QUAD_LRS)):\n",
        "    fit_data[:, :, 2 + i] = np.array(fit_normal(MNISTLoss, create_MNISTNet(nn.ReLU()), opt, lr=lr, **extra_kwargs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0roBZt1mBACi"
      },
      "source": [
        "np.save('mnist_fit_data_relu', fit_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76ByDSE2A_-_"
      },
      "source": [
        "names = ['LSTM', 'ADAM', 'RMSprop', 'SGD', 'NAG']\n",
        "for i, name in enumerate(names):\n",
        "    print('{}: {:.2f}'.format(name, np.mean(fit_data, axis=0)[-1, i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTdoICLGA_74"
      },
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "for i, opt in enumerate(names):\n",
        "    plt.plot(np.mean(fit_data[:,:,i], axis=0), label=opt)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('MNIST, ReLU')\n",
        "plt.legend()\n",
        "plt.yscale('log')\n",
        "plt.grid()\n",
        "plt.savefig('mnist_relu.pdf')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUz_5accA_5Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}