# -*- coding: utf-8 -*-
"""Копия блокнота "MLDM-2021_seminar01_homework.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gRBQGaFl55h6Azn3CGCasP_pFbOzxc3d

## Please, fill in before you start:

First Name: Anna

Last Name: Makarova

Group: MKH201 Cognitive sciences and technologies

### After the assigment is done, please, push it to a [private GitHub repository](https://docs.github.com/en/github/administering-a-repository/managing-repository-settings/setting-repository-visibility) and invite [SiLiKhon](https://github.com/SiLiKhon), [dMeVdok](https://github.com/dmevdok), [oleges1](https://github.com/oleges1) and [rogachevai](https://github.com/rogachevai) [as collaborators](https://docs.github.com/en/account-and-profile/setting-up-and-managing-your-github-user-account/managing-access-to-your-personal-repositories/inviting-collaborators-to-a-personal-repository).

Get the Titanic dataset:
"""

!wget https://raw.githubusercontent.com/HSE-LAMBDA/MLDM-2021/main/01-intro/train.csv

import pandas as pd
data = pd.read_csv("train.csv", index_col='PassengerId')
data.head()

"""#### About the data
Here's some of the columns
* Name - a string with person's full name
* Survived - 1 if a person survived the shipwreck, 0 otherwise.
* Pclass - passenger class. Pclass == 3 is cheap'n'cheerful, Pclass == 1 is for moneybags.
* Sex - a person's gender
* Age - age in years, if available
* SibSp - number of siblings on a ship
* Parch - number of parents on a ship
* Fare - ticket cost
* Embarked - port where the passenger embarked
 * C = Cherbourg; Q = Queenstown; S = Southampton

## Task 1 (1 point)
"""

# Compute survival rate for each of the three passenger classes (`Pclass` = 1, 2 and 3)
# (can you do it using groupby?)

# compute survival rate for each class
first_class_survival_rate = len(data.loc[(data['Pclass'] == 1) & (data['Survived'] == 1)])/len(data.loc[(data['Pclass'] == 1)])
second_class_survival_rate = len(data.loc[(data['Pclass'] == 2) & (data['Survived'] == 1)])/len(data.loc[(data['Pclass'] == 2)])
third_class_survival_rate = len(data.loc[(data['Pclass'] == 3) & (data['Survived'] == 1)])/len(data.loc[(data['Pclass'] == 3)])
print(first_class_survival_rate)
print(second_class_survival_rate)
print(third_class_survival_rate)

first_class = data.groupby(['Survived','Pclass'])

data.groupby(['Survived','Pclass']).size()

"""## Task 2 (1 point)"""

# Plot the average number of parents onboard (`Parch`) as a function of the 
# number of siblings onboard (`SibSp`)

import matplotlib.pyplot as plt

# calculate average number of parents onboard
avrg_num_parents = []
for number_of_sib in range(data['SibSp'].max()+1):
  avrg_num_parents.append(data.loc[(data['SibSp'] == number_of_sib)]['Parch'].mean())
print(avrg_num_parents)

# plot the function
import numpy as np
x = np.arange(data['SibSp'].max()+1)

plt.scatter(x, avrg_num_parents)
plt.plot(x, avrg_num_parents, c = 'black')

plt.title('Number of parents and siblings')
plt.xlabel('Number of siblings')
plt.ylabel('Average number of siblings');

"""## Task 3 (2 points)"""

# 1, 5, 7,

# Build a model with KNeighborsClassifier to get the accuracy of
# at least 0.75 on the validation part of the dataset

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

def feature_selection_and_preprocessing(dataset):
  dataset['Sex'] = np.where(dataset['Sex'] == 'male', 0, dataset['Sex'])
  dataset['Sex'] = np.where(dataset['Sex'] == 'female', 1, dataset['Sex'])

  dataset['Age'] = dataset['Age'].fillna(value=dataset['Age'].mean())
  features = dataset[['Sex', 'Age']].copy()

  return features
  # E.g.:
# features = dataset[["Fare", "Parch"]].copy()
# features["Fare"] /= features["Fare"].mean()
# features['Fare'] *= 1
# return features

model = KNeighborsClassifier(
    n_neighbors=7
)


# Validation code (do not touch)
data = pd.read_csv("train.csv", index_col='PassengerId')
data_train = data.iloc[:-100]
data_test = data.iloc[-100:]

model.fit(
    feature_selection_and_preprocessing(
        data_train.drop('Survived', axis=1)
    ),
    data_train['Survived']
)

test_predictions = model.predict(
    feature_selection_and_preprocessing(
        data_test.drop('Survived', axis=1)
    )
)
print("Test accuracy:", accuracy_score(
    data_test['Survived'],
    test_predictions
))

"""## Task 4 (2 points)

Check how your model from the previous task performs on randomized splits to train / test (with test set of size 100). Plot the histogram of the test error distribution.

*Hint: check sklearn's `sklearn.model_selection.train_test_split` function.*
"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(data_train)